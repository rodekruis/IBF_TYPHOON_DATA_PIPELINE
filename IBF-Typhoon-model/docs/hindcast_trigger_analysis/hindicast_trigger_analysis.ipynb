{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import libraries from local directory\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from os.path import relpath\n",
    "from pybufrkit.decoder import Decoder\n",
    "from geopandas.tools import sjoin\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.ops import nearest_points \n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from dateutil import rrule\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "decoder = Decoder()\n",
    "#wor_dir=\"/home/fbf/\"\n",
    "wor_dir=\"C:\\\\Users\\\\ATeklesadik\\\\OneDrive - Rode Kruis\\\\Documents\\\\documents\\\\Typhoon-Impact-based-forecasting-model\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hindcast analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    " \n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    " \n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "from dateutil import rrule\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#wor_dir='c:/Users/ATeklesadik/OneDrive - Rode Kruis/Documents/documents/Typhoon-Impact-based-forecasting-model/'\n",
    "wor_dir=\"C:\\\\Users\\\\ATeklesadik\\\\OneDrive - Rode Kruis\\\\Documents\\\\documents\\\\Typhoon-Impact-based-forecasting-model\"\n",
    "typhoon_metadata_filename = os.path.join(wor_dir, \"IBF-Typhoon-model\\\\data\\\\metadata_typhoons.csv\")\n",
    "typhoon_metadata = pd.read_csv(typhoon_metadata_filename, delimiter=\",\")\n",
    "\n",
    " \n",
    "typhoon_metadata['landfall_dd_hh']=typhoon_metadata.apply(lambda x:x.landfalldate +' '+ x.landfall_time,axis=1\t)\n",
    "typhoon_metadata['landfall_']=typhoon_metadata.apply(lambda x:datetime.strptime(x.landfall_dd_hh, '%d/%m/%Y %H:%M:00'),axis=1\t)\n",
    " \n",
    " \n",
    "typhoon_metadata=typhoon_metadata.query('typhoon in @list_of_events')\n",
    "typhoon_metadata['landfall_dd_hh']=typhoon_metadata.apply(lambda x:x.landfalldate +' '+ x.landfall_time,axis=1\t)\n",
    "typhoon_metadata['landfall_']=typhoon_metadata.apply(lambda x:datetime.strptime(x.landfall_dd_hh, '%d/%m/%Y %H:%M:00'),axis=1\t)\n",
    "typhoon_metadata=typhoon_metadata.query('typhoon in @list_of_events')\n",
    "\n",
    "list_of_events=[\n",
    "    'krosa2013','fengshen2008','haiyan2013','rammasun2014', 'hagupit2014',  'haima2016','phanfone2019', \n",
    "    'nock-ten2016' \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "typhoon_metadata[\"international\"] = typhoon_metadata['typhoon'].apply(lambda x:x[:-4])\n",
    "typhoon_metadata[\"year\"] = typhoon_metadata['landfall_'].apply(lambda x:x.year)\n",
    "\n",
    " \n",
    " \n",
    "df_typhoons = typhoon_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download and process input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def hour_rounder(t):\n",
    "    # Rounds to nearest hour by adding a timedelta hour if minute >= 30\n",
    "    return (t.replace(second=0, microsecond=0, minute=0, hour=t.hour) - timedelta(hours=t.hour-6))\n",
    "\n",
    "#from constants import save_dir\n",
    "\n",
    "\n",
    "tem_dir=\"C:\\\\Users\\\\ATeklesadik\\\\OneDrive - Rode Kruis\\\\Documents\\\\documents\\\\Typhoon-Impact-based-forecasting-model\\\\IBF-Typhoon-model\\\\data\\\\wind_data\\\\\"\n",
    "save_dir=os.path.join(tem_dir,'temp')\n",
    "save_dir = Path(save_dir)\n",
    "\n",
    "email = input('email: ')\n",
    "pswd = input('password: ')\n",
    "\n",
    "values = {'email' : email, 'passwd' : pswd, 'action' : 'login'}\n",
    "login_url = 'https://rda.ucar.edu/cgi-bin/login'\n",
    "\n",
    "ret = requests.post(login_url, data=values)\n",
    "\n",
    "\n",
    "values = {'email' : email, 'passwd' : pswd, 'action' : 'login'}\n",
    "login_url = 'https://rda.ucar.edu/cgi-bin/login'\n",
    "\n",
    "ret = requests.post(login_url, data=values)\n",
    "if ret.status_code != 200:\n",
    "    print('Bad Authentication')\n",
    "    print(ret.text)\n",
    "    exit(1)\n",
    "\n",
    "dspath = 'https://rda.ucar.edu/data/ds330.3/'\n",
    "\n",
    "verbose = True\n",
    "\n",
    "\n",
    "for date,row in typhoon_metadata.iterrows():\n",
    "    start_date =row['landfall_'] - timedelta(hours=72)\n",
    "    start_date =hour_rounder(start_date)\n",
    "    end_date = row['landfall_'] \n",
    "    \n",
    "    date_list = rrule.rrule(rrule.HOURLY, \n",
    "                            dtstart=start_date, \n",
    "                            until=end_date,\n",
    "                            interval=6)\n",
    "    \n",
    "    for date in date_list:\n",
    "        ymd = date.strftime(\"%Y%m%d\")\n",
    "        ymdhms = date.strftime(\"%Y%m%d%H%M%S\")\n",
    "        server = \"test\" if date < datetime(2008, 8, 1) else \"prod\"\n",
    "        file = f'ecmf/{date.year}/{ymd}/z_tigge_c_ecmf_{ymdhms}_ifs_glob_{server}_all_glo.xml'\n",
    "        filename = dspath + file\n",
    "        outfile = save_dir / \"xml\" / os.path.basename(filename)\n",
    "        # Don't download if exists already\n",
    "        if outfile.exists():\n",
    "            if verbose:\n",
    "                print(f'{file} already exists')\n",
    "            continue\n",
    "        req = requests.get(filename, cookies = ret.cookies, allow_redirects=True)\n",
    "        if req.status_code != 200:\n",
    "            if verbose:\n",
    "                print(f'{file} invalid URL')\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(f'{file} downloading')\n",
    "        open(outfile, 'wb').write(req.content)\n",
    "  \n",
    "\n",
    "def xml2csv(filename):\n",
    "    print(f\"{filename}\")\n",
    "    try:\n",
    "        tree = ET.parse(filename)\n",
    "    except ET.ParseError:\n",
    "        print(\"Error with file, skipping\")\n",
    "        return\n",
    "    root = tree.getroot()\n",
    "\n",
    "    prod_center=root.find('header/productionCenter').text\n",
    "    baseTime=root.find('header/baseTime').text\n",
    "\n",
    "    ## Create one dictonary for each time point, and append it to a list\n",
    "    for members in root.findall('data'):\n",
    "        mtype=members.get('type')\n",
    "        if mtype not in ['forecast', 'ensembleForecast']:\n",
    "            continue\n",
    "        for members2 in members.findall('disturbance'):\n",
    "            cyclone_name = [name.text.lower().strip() for name in members2.findall('cycloneName')]\n",
    "            if not cyclone_name:\n",
    "                continue\n",
    "            cyclone_name = cyclone_name[0].lower()\n",
    "            if cyclone_name not in list(df_typhoons[\"international\"]):\n",
    "                continue\n",
    "            print(f\"Found typhoon {cyclone_name}\")\n",
    "            for members3 in members2.findall('fix'):\n",
    "                tem_dic = {}\n",
    "                tem_dic['mtype']=[mtype]\n",
    "                tem_dic['product']=[re.sub('\\s+',' ',prod_center).strip().lower()]\n",
    "                tem_dic['cyc_number'] = [name.text for name in members2.findall('cycloneNumber')]\n",
    "                tem_dic['ensemble']=[members.get('member')]\n",
    "                tem_dic['speed'] = [name.text for name in members3.findall('cycloneData/maximumWind/speed')]\n",
    "                tem_dic['pressure'] = [name.text for name in members3.findall('cycloneData/minimumPressure/pressure')]\n",
    "                time = [name.text for name in members3.findall('validTime')]\n",
    "                tem_dic['time'] = ['/'.join(time[0].split('T')[0].split('-'))+', '+time[0].split('T')[1][:-1]]\n",
    "                tem_dic['lat'] = [name.text for name in members3.findall('latitude')]\n",
    "                tem_dic['lon']= [name.text for name in members3.findall('longitude')]\n",
    "                tem_dic['lead_time']=[members3.get('hour')]\n",
    "                tem_dic['forecast_time'] = ['/'.join(baseTime.split('T')[0].split('-'))+', '+baseTime.split('T')[1][:-1]]\n",
    "                tem_dic1 = dict( [(k,''.join(str(e).lower().strip() for e in v)) for k,v in tem_dic.items()])\n",
    "                # Save to CSV\n",
    "                outfile = save_dir / f\"csv/{cyclone_name}_all.csv\"\n",
    "                pd.DataFrame(tem_dic1, index=[0]).to_csv(outfile, mode='a', header=not os.path.exists(outfile), index=False)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# Get list of filenames\n",
    "filename_list = sorted(list(Path(save_dir / \"xml\").glob('*.xml')))\n",
    "\n",
    "for filename in filename_list:\n",
    "    xml2csv(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate wind field data for each manucipality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typhoon_forecast_dict={}\n",
    "\n",
    "list_of_events=['nock-ten2016']\n",
    "filename_list1=[fname for fname in filename_list if os.path.basename(fname) in ['nock-ten_2016.csv']]\n",
    "\n",
    "import time\n",
    "for i,row, in typhoon_metadata.iterrows():\n",
    "    landfall_=row['landfall_']\n",
    "    landfall_time=datetime.strptime(landfall_.strftime('%Y/%m/%d, %H:%M:%S'), '%Y/%m/%d,  %H:%M:%S') \n",
    "    typhoon_name=row['typhoon']\n",
    "    if typhoon_name in list_of_events:#'hagupit2014':  \n",
    "        tr_data=[tr for tr in DATA_YPHOON1 if (tr.name+tr.sid[:4])==typhoon_name]\n",
    "        forecast_time_list=sorted(list(set([tr.forecast_time  for tr in tr_data])), reverse=False)\n",
    "        for forecast_time in forecast_time_list:\n",
    "            lead_time1=(landfall_time-forecast_time).total_seconds() / 3600 \n",
    "            if lead_time1 > 23:                \n",
    "                tr_data2=[tr for tr in tr_data if tr.forecast_time==forecast_time]\n",
    "                tracks = TCTracks()\n",
    "                # Select typhoons that are in the typhoon event sheet\n",
    "                tracks.data =tr_data2 # [ tr for tr in DATA_YPHOON if (tr.name + tr.sid[:4]) in typhoon_events]\n",
    "                #tracks.data = [adjust_tracks(tr) for tr in Typhoons.data]\n",
    "                tracks.equal_timestep(0.5)\n",
    "                \n",
    "                TYphoon = TropCyclone()\n",
    "                TYphoon.set_from_tracks(tracks, cent, store_windfields=True,metric=\"geosphere\")\n",
    "                windfield=TYphoon.windfields\n",
    "                #calculate wind field for each ensamble members \n",
    "                df = pd.DataFrame(data=cent.coord)\n",
    "                df[\"centroid_id\"] = \"id\" + (df.index).astype(str)\n",
    "                centroid_idx = df[\"centroid_id\"].values\n",
    "                ncents = cent.size\n",
    "                df = df.rename(columns={0: \"lat\", 1: \"lon\"})\n",
    "                threshold = 0.1\n",
    "\n",
    "                df_ = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "                df_.crs = {\"init\": \"epsg:4326\"}\n",
    "                df_ = df_.to_crs(\"EPSG:4326\")\n",
    "                df_admin = sjoin(df_, admin, how=\"left\")\n",
    "                # To remove points that are in water\n",
    "                df_admin = df_admin.dropna()\n",
    "\n",
    "                list_intensity = []\n",
    "                distan_track = []\n",
    "\n",
    "                list_intensity=[]\n",
    "                distan_track=[]\n",
    "                \n",
    "                for i in range(len(tracks.data)):        \n",
    "                    nsteps = windfield[i].shape[0]\n",
    "                    tr=tracks.data[i]\n",
    "                    centroid_id = np.tile(centroid_idx, nsteps)\n",
    "                    intensity_3d = windfield[i].toarray().reshape(nsteps, ncents, 2)\n",
    "                    intensity = np.linalg.norm(intensity_3d, axis=-1).ravel()\n",
    "                    timesteps = np.repeat(tracks.data[i].time.values, ncents)\n",
    "                    timesteps = timesteps.reshape((nsteps, ncents)).ravel()\n",
    "                    inten_tr = pd.DataFrame({\n",
    "                            'centroid_id': centroid_id,\n",
    "                            'value': intensity,\n",
    "                            'timestamp': timesteps,})\n",
    "                    \n",
    "                    inten_tr = inten_tr[inten_tr.value > threshold]\n",
    "                    \n",
    "                    inten_tr['storm_id'] = tr.sid\n",
    "                    #inten_tr['ens_id'] =tr.sid+'_'+str(tr.ensemble_number)\n",
    "                    inten_tr['name'] = tr.name\n",
    "                    \n",
    "                    inten_tr = (pd.merge(inten_tr, df_admin, how='outer', on='centroid_id')\n",
    "                                .dropna()\n",
    "                                .groupby(['adm3_pcode'], as_index=False)\n",
    "                                .agg({\"value\": ['count', 'max']}))\n",
    "                    \n",
    "                    \n",
    "                    inten_tr.columns = [x for x in ['adm3_pcode', 'value_count', 'v_max']]\n",
    "                    inten_tr['storm_id'] = tr.sid\n",
    "                    inten_tr['name'] = tr.name\n",
    "                    inten_tr['forecast_time']=tr.forecast_time\n",
    "                    inten_tr['lead_time']=lead_time1\n",
    "                    list_intensity.append(inten_tr)\n",
    " \n",
    "                \n",
    "        \n",
    "                    distan_track1=[]\n",
    "                    \n",
    "                    for index, row in df.iterrows():\n",
    "                        dist=np.min(np.sqrt(np.square(tr.lat.values-row['lat'])+np.square(tr.lon.values-row['lon'])))\n",
    "                        distan_track1.append(dist*111)\n",
    "                    dist_tr = pd.DataFrame({'centroid_id': centroid_idx,'value': distan_track1})\n",
    "\n",
    "                    dist_tr = (pd.merge(dist_tr, df_admin, how='outer', on='centroid_id')\n",
    "                                .dropna()\n",
    "                                .groupby(['adm3_pcode'], as_index=False)\n",
    "                                .agg({'value': 'min'}))\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    dist_tr.columns = [x for x in ['adm3_pcode', 'dis_track_min']]  # join_left_df_.columns.ravel()]\n",
    "                    dist_tr['storm_id'] = tr.sid\n",
    "                    #dist_tr['name'] = tr.name\n",
    "                    distan_track.append(dist_tr)\n",
    "    \n",
    "                df_intensity_ = pd.concat(list_intensity)\n",
    "                distan_track1 = pd.concat(distan_track)\n",
    "\n",
    "                typhhon_df =  pd.merge(df_intensity_, distan_track1,  how='left', on=['adm3_pcode','storm_id']) \n",
    "\n",
    "                # Check if there are duplicates for municipality and storm_id\n",
    "\n",
    "\n",
    "                #distan_track1 = pd.concat(distan_track)\n",
    "\n",
    "                #typhhon_df = df_intensity_ #pd.merge(df_intensity_, distan_track1,  how='left', on=['adm3_pcode','storm_id']) \n",
    "\n",
    "                # Check if there are duplicates for municipality and storm_id\n",
    "\n",
    "                duplicate = typhhon_df[\n",
    "                    typhhon_df.duplicated(subset=[\"adm3_pcode\", \"storm_id\"], keep=False)\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                file_name = (f\"IBF-typhoon-model\\\\data\\\\wind_data\\\\hindcast\\\\hindcast_{typhoon_name}_{lead_time1}.csv\")\n",
    "\n",
    "                path = os.path.join(wor_dir, file_name)\n",
    "\n",
    "                typhhon_df.to_csv(path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# additional libraries\n",
    "import cartopy.crs as ccrs\n",
    "import cftime\n",
    "import geopandas as gpd\n",
    "import matplotlib.cm as cm_mp\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io.matlab as matlab\n",
    "from shapely.geometry import Point, LineString, MultiLineString\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "import statsmodels.api as sm\n",
    "import xarray as xr\n",
    "\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    "    KFold)\n",
    " \n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, \n",
    "    mean_squared_error,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    make_scorer)\n",
    "\n",
    "import xgboost as xgb\n",
    "import glob\n",
    "\n",
    "from geopandas.tools import sjoin\n",
    " \n",
    "from shapely.ops import nearest_points\n",
    "import datetime as datetime\n",
    "\n",
    "wor_dir=\"C:\\\\Users\\\\ATeklesadik\\\\OneDrive - Rode Kruis\\\\Documents\\\\documents\\\\Typhoon-Impact-based-forecasting-model\"\n",
    "f_path = os.path.join(wor_dir,\"IBF-typhoon-model/data/all_predisaster_indicators.csv\") \n",
    "df_predisasters = pd.read_csv(f_path)\n",
    "\n",
    "\n",
    "def division(x, y):\n",
    "    try:\n",
    "        value =100* (x / y)\n",
    "        \n",
    "    except:\n",
    "        value = np.nan\n",
    "    \n",
    "    return 100 if value>100 else value\n",
    "\n",
    "\n",
    "# Setting the new damage threshold\n",
    "\n",
    "df_predisasters[\"vulnerable_groups\"] = df_predisasters.apply(lambda x: division(x[\"vulnerable_groups\"], x[\"Total Pop\"]), axis=1).values\n",
    "#df_total[\"pantawid_pamilya_beneficiary\"] = df_total.apply(lambda x: division(x[\"Total # of Active HHs\"], x[\"Housing Units\"]), axis=1).values\n",
    "df_predisasters[\"pantawid_pamilya_beneficiary\"] = df_predisasters.apply(lambda x: division(x[\"pantawid_total_pop\"], x[\"Total Pop\"]), axis=1).values\n",
    "#%%\n",
    "#############################\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "combined_input_data=pd.read_csv(os.path.join(wor_dir,\"IBF-typhoon-model/data/model_input/df_modelinput_sep.csv\")) \n",
    "tphoon_events=combined_input_data[['typhoon','DAM_perc_dmg']].groupby('typhoon').size().to_dict()\n",
    "### for hyper parameter optimization we looped over typhoon events with at least 100 data entries  \n",
    "\n",
    " \n",
    "def set_zeros(x):\n",
    "    x_max = 25\n",
    "    y_max = 50    \n",
    "    v_max = x[0]\n",
    "    rainfall_max = x[1]\n",
    "    damage = x[2]\n",
    "    if pd.notnull(damage):\n",
    "        value = damage\n",
    "    elif v_max > x_max or rainfall_max > y_max:\n",
    "        value =damage\n",
    "    elif (v_max < np.sqrt((1- (rainfall_max**2/y_max ** 2))*x_max ** 2)):\n",
    "        value = 0\n",
    "    #elif ((v_max < x_max)  and  (rainfall_max_6h < y_max) ):\n",
    "    #elif (v_max < x_max ):\n",
    "    #value = 0\n",
    "    else:\n",
    "        value = np.nan\n",
    "    return value\n",
    "\n",
    "combined_input_data[\"DAM_perc_dmg\"] = combined_input_data[[\"HAZ_v_max\", \"HAZ_rainfall_Total\", \"DAM_perc_dmg\"]].apply(set_zeros, axis=\"columns\")\n",
    "\n",
    "\n",
    "selected_features_xgb_regr= [#'HAZ_rainfall_max_24h',\n",
    "                                       'HAZ_v_max',\n",
    "                                       'HAZ_dis_track_min',\n",
    "                                       'TOP_mean_slope',\n",
    "                                       'TOP_mean_elevation_m',\n",
    "                                       'TOP_ruggedness_stdev',\n",
    "                                       'TOP_mean_ruggedness',\n",
    "                                       'TOP_slope_stdev',\n",
    "                                       'VUL_poverty_perc',\n",
    "                                       'GEN_with_coast',\n",
    "                                       'VUL_Housing_Units',\n",
    "                                       'VUL_StrongRoof_StrongWall',\n",
    "                                       'VUL_StrongRoof_LightWall',\n",
    "                                       'VUL_StrongRoof_SalvageWall',\n",
    "                                       'VUL_LightRoof_StrongWall',\n",
    "                                       'VUL_LightRoof_LightWall',\n",
    "                                       'VUL_SalvagedRoof_StrongWall',\n",
    "                                       'VUL_SalvagedRoof_LightWall',\n",
    "                                       'VUL_SalvagedRoof_SalvageWall',\n",
    "                                       'VUL_vulnerable_groups',\n",
    "                                       'VUL_pantawid_pamilya_beneficiary']\n",
    "\n",
    "\n",
    "\n",
    "# split data into train and test sets\n",
    "\n",
    "SEED2 = 314159265\n",
    "SEED = 31 \n",
    " \n",
    "test_size = 0.1\n",
    "\n",
    "# Full dataset for feature selection\n",
    "\n",
    "combined_input_data_ = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "\n",
    "\n",
    "X = combined_input_data_[selected_features_xgb_regr]\n",
    "y = combined_input_data_[\"DAM_perc_dmg\"]\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=SEED)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from sklearn.metrics import mean_absolute_error\n",
    "\n",
    " \n",
    "reg = xgb.XGBRegressor(base_score=0.5,\n",
    "             booster='gbtree', \n",
    "             subsample=0.8,\n",
    "             eta=0.05,\n",
    "             max_depth=8, \n",
    "             colsample_bylevel=1,\n",
    "             colsample_bynode=1, \n",
    "             colsample_bytree=1,\n",
    "             early_stopping_rounds=10, \n",
    "             eval_metric=mean_absolute_error,\n",
    "             gamma=1, \n",
    "             objective='reg:squarederror',\n",
    "             gpu_id=-1, \n",
    "             grow_policy='depthwise', \n",
    "             learning_rate=0.025,\n",
    "             min_child_weight=1,\n",
    "             n_estimators=200,        \n",
    "             random_state=42,\n",
    "             tree_method=\"hist\",\n",
    "             )\n",
    "\n",
    "eval_set=[(X_train, y_train), ( X_test, y_test)]\n",
    "\n",
    "reg.fit(X, y, eval_set=eval_set)\n",
    "\n",
    "#%%%\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "#%%%%%%%%\n",
    "\n",
    "#Adding the wind data only for events with impact data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dref_probabilities = {\n",
    "            \"100k\": [100000, 0.5],\n",
    "            \"80k\": [80000, 0.6],\n",
    "            \"70k\": [70000, 0.7],\n",
    "            \"50k\": [50000, 0.8],\n",
    "            \"30k\": [30000, 0.9],\n",
    "        }\n",
    "\n",
    "\n",
    "dref_probabilities_10 = {\n",
    "            \"33\": [33,'Moderate'],\n",
    "            \"50\": [50,'High'],\n",
    "            \"70\":[70,'Very High'],\n",
    "        }\n",
    "\n",
    "\n",
    "cerf_probabilities = {\n",
    "    \"80k\": [80000, 0.5],\n",
    "    \"50k\": [50000, 0.6],\n",
    "    \"30k\": [30000, 0.7],\n",
    "    \"10k\": [10000, 0.8],\n",
    "    \"5k\": [5000, 0.95],\n",
    "}\n",
    "        \n",
    "        \n",
    "START_probabilities = {\n",
    "    'PH166700000':{\n",
    "        \"8k\": [8000, 0.8],\n",
    "        \"10k\": [10000, 0.8],\n",
    "        \"15k\": [15000, 0.7],\n",
    "        \"20k\": [20000, 0.5],\n",
    "        \"25k\": [25000, 0.5],    \n",
    "         \n",
    "        },\n",
    "    'PH021500000':{\n",
    "        \"16k\": [16000, 0.8],\n",
    "        \"18k\": [18000, 0.8],\n",
    "        \"20k\": [20000, 0.7],\n",
    "        \"25k\": [25000, 0.5],\n",
    "        \"28.5k\": [28500, 0.5], \n",
    "   \n",
    "        },\n",
    "    'PH082600000':{\n",
    "        \"18k\": [18000, 0.8],\n",
    "        \"20k\": [20000, 0.8],\n",
    "        \"25k\": [25000, 0.7],\n",
    "        \"30k\": [30000, 0.6],\n",
    "        \"40k\": [40000, 0.5],\n",
    "        },\n",
    "}\n",
    "\n",
    "# Only select regions 5 and 8\n",
    "cerf_regions = [\"PH05\", \"PH08\"]\n",
    "\n",
    "start_regions = {'PH166700000':'surigaodelnorte', 'PH021500000':'cagayan', 'PH082600000':'EasternSamar'}\n",
    "\n",
    "cerf_trigger_list={}\n",
    "start_trigger_dic={}\n",
    "DREF_trigger_list={}\n",
    "DREF_trigger_list_10={}\n",
    "\n",
    "\n",
    "file_name = (\"IBF-typhoon-model\\\\data\\\\wind_data\\\\output\\\\\")#historical_events_windgrid_output.csv\")\n",
    "#historical_events_windgrid_output_hrs\n",
    "wind_path = os.path.join(wor_dir, file_name)\n",
    "#wind_path = os.path.join(wor_dir,\"data/wind_data/output_storm2\")\n",
    "#os.chdir(wind_path)\n",
    "\n",
    "dfs=[]\n",
    "def Number_affected(x):\n",
    "    return np.exp(6.80943612231606) * x ** 0.46982114400549513\n",
    "    \n",
    "def division(x, y):\n",
    "    try:\n",
    "        value = int((x * y)/100)\n",
    "        \n",
    "    except:\n",
    "        value = np.nan\n",
    "    \n",
    "    return x if value>x else value \n",
    "\n",
    "filename_list_wind = sorted(list(Path(os.path.join(wor_dir ,\"IBF-typhoon-model/data/wind_data/hindcast\")).glob('*.csv')))\n",
    "inx_=0\n",
    "index2=0\n",
    "inx_3=0\n",
    " \n",
    "for file_wind in filename_list_wind:\n",
    "    #windfiles = glob.glob(f'*_{index}.csv')\n",
    "    index=os.path.basename(file_wind)\n",
    "    index_=index.split('.')[0]\n",
    "    event_name=index.split('_')[1]\n",
    "    lead_time2=index.split('_')[-1].split('.')[0]\n",
    "    #f_path = os.path.join(wind_path,f'{index}.csv')\n",
    " \n",
    "    \n",
    "    df_temp = pd.read_csv(file_wind)\n",
    "    \n",
    "    #if len(df_temp.index)>1:\n",
    "    df_temp.rename(columns={'adm3_pcode':'Mun_Code','name':'typhoon'},inplace=True)\n",
    "        \n",
    "    df_wind=df_temp[['storm_id','Mun_Code','v_max','typhoon','forecast_time','dis_track_min']]#.groupby(\"Mun_Code\").agg({'v_max':'max'})\n",
    "     \n",
    " \n",
    "    df_wind_final = df_wind#pd.concat(dfs,axis=0, join='outer')    \n",
    "    df_total=pd.merge(df_wind_final, df_predisasters,  how='left', left_on='Mun_Code', right_on = 'Mun_Code')\n",
    "    df_total=df_total[df_total['v_max'].notnull()]\n",
    "    df_total.rename(columns ={           'v_max':'HAZ_v_max',\n",
    "                                        'dis_track_min':'HAZ_dis_track_min',\n",
    "                                        'landslide_per':'GEN_landslide_per',\n",
    "                                        'stormsurge_per':'GEN_stormsurge_per',\n",
    "                                        'Bu_p_inSSA':'GEN_Bu_p_inSSA',\n",
    "                                        'Bu_p_LS':'GEN_Bu_p_LS',\n",
    "                                         'Red_per_LSbldg':'GEN_Red_per_LSbldg',\n",
    "                                        'Or_per_LSblg':'GEN_Or_per_LSblg',\n",
    "                                         'Yel_per_LSSAb':'GEN_Yel_per_LSSAb',\n",
    "                                        'RED_per_SSAbldg':'GEN_RED_per_SSAbldg',\n",
    "                                         'OR_per_SSAbldg':'GEN_OR_per_SSAbldg',\n",
    "                                        'Yellow_per_LSbl':'GEN_Yellow_per_LSbl',\n",
    "                                         'mean_slope':'TOP_mean_slope',\n",
    "                                        'mean_elevation_m':'TOP_mean_elevation_m',\n",
    "                                         'ruggedness_stdev':'TOP_ruggedness_stdev',\n",
    "                                        'mean_ruggedness':'TOP_mean_ruggedness',\n",
    "                                         'slope_stdev':'TOP_slope_stdev',\n",
    "                                         'poverty_perc':'VUL_poverty_perc',\n",
    "                                        'with_coast':'GEN_with_coast',\n",
    "                                         'coast_length':'GEN_coast_length',\n",
    "                                         'Housing Units':'VUL_Housing_Units',\n",
    "                                        'Strong Roof/Strong Wall':\"VUL_StrongRoof_StrongWall\",\n",
    "                                        'Strong Roof/Light Wall':'VUL_StrongRoof_LightWall',\n",
    "                                        'Strong Roof/Salvage Wall':'VUL_StrongRoof_SalvageWall',\n",
    "                                        'Light Roof/Strong Wall':'VUL_LightRoof_StrongWall',\n",
    "                                        'Light Roof/Light Wall':'VUL_LightRoof_LightWall',\n",
    "                                        'Light Roof/Salvage Wall':'VUL_LightRoof_SalvageWall',\n",
    "                                        'Salvaged Roof/Strong Wall':'VUL_SalvagedRoof_StrongWall',\n",
    "                                        'Salvaged Roof/Light Wall':'VUL_SalvagedRoof_LightWall',\n",
    "                                        'Salvaged Roof/Salvage Wall':'VUL_SalvagedRoof_SalvageWall',\n",
    "                                        'vulnerable_groups':'VUL_vulnerable_groups',\n",
    "                                        'pantawid_pamilya_beneficiary':'VUL_pantawid_pamilya_beneficiary'},inplace=True)\n",
    "    #df_total = df_total.filter(selected_cols)\n",
    "    X_all = df_total[selected_features_xgb_regr]\n",
    "    y_pred = reg.predict(X_all)\n",
    "    \n",
    "    \n",
    "    df_total['DMG_predicted']=y_pred \n",
    "    y_pred[y_pred<10]=0\n",
    "    y_pred[y_pred>0]=1\n",
    "    df_total['Trigger']=y_pred \n",
    "    \n",
    "    IMPACT_DF1 = pd.merge(df_total, df_predisasters[['Housing Units','Mun_Code']],  how='left', left_on='Mun_Code', right_on = 'Mun_Code') \n",
    "    IMPACT_DF1[\"number_dmg_Build_prediction\"] = IMPACT_DF1.apply(lambda x: division(x[\"Housing Units\"], x[\"DMG_predicted\"]), axis=1).values\n",
    "  \n",
    "    \n",
    "    IMPACT_DF1[\"number_affected_pop__prediction\"] = IMPACT_DF1.apply(lambda x: Number_affected(x[\"number_dmg_Build_prediction\"]), axis=1).values\n",
    "    \n",
    "    impact_scenarios=['number_dmg_Build_prediction','number_affected_pop__prediction']\n",
    "    \n",
    "    IMPACT_DF1[impact_scenarios] = IMPACT_DF1[impact_scenarios].astype('int') \n",
    "    IMPACT_DF1['Prov_Code']=IMPACT_DF1.apply(lambda x:str(x.Mun_Code[:6])+'00000',axis=1)\n",
    "    IMPACT_DF1['typhoon_year']=IMPACT_DF1.apply(lambda x:x.typhoon+str(x.storm_id[:4]),axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    impact_path = os.path.join(wor_dir,\"IBF-typhoon-model/data/results/hindcast\",f\"{index}impact.csv\")\n",
    " \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    IMPACT_DF12=IMPACT_DF1[['Prov_Code','Mun_Code','storm_id','typhoon_year', 'forecast_time','DMG_predicted','Trigger','number_dmg_Build_prediction','number_affected_pop__prediction']]\n",
    "    IMPACT_DF12.to_csv(impact_path) \n",
    "    \n",
    "    dfs.append(IMPACT_DF12)\n",
    "    \n",
    "    \n",
    "    probability_impact=IMPACT_DF12.groupby(['storm_id','typhoon_year','forecast_time']).agg(NUmber_of_affected_municipality=('Mun_Code','count'),\n",
    "                                        average_ML_Model=('DMG_predicted', 'mean'),\n",
    "                                        Trigger_ML_Model=('Trigger', sum),\n",
    "                                        Total_affected_ML_Model=('number_affected_pop__prediction', sum),\n",
    "                                        Total_buildings_ML_Model=('number_dmg_Build_prediction', sum)).sort_values(by='Total_buildings_ML_Model',ascending=False).reset_index()\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    ### DREF trigger based on 10% damage per manucipality  \n",
    "    trigger_status1={}\n",
    "    probability_impact['Trigger']=probability_impact.apply(lambda x:1 if x.Trigger_ML_Model>2 else 0,axis=1)\n",
    "    agg_impact_10 = probability_impact[\"Trigger\"].values\n",
    "    trigger_stat_dref10 = 100*(sum(agg_impact_10) /len(agg_impact_10))\n",
    "    \n",
    "    for key, values in dref_probabilities_10.items():\n",
    "        inx_3=inx_3+1\n",
    "        dref_trigger_status10 = {}\n",
    "        thershold=values[1]\n",
    "        if  (trigger_stat_dref10 > values[0]):\n",
    "            trigger_stat_1=True\n",
    " \n",
    "        else:\n",
    "            trigger_stat_1=False\n",
    " \n",
    "        dref_trigger_status10['lead_time'] = int(lead_time2)-10\n",
    "        dref_trigger_status10['triggered_prob'] = thershold \n",
    "        dref_trigger_status10['EVENT'] = event_name \n",
    "        dref_trigger_status10['trigger_stat'] = trigger_stat_1         \n",
    "        trigger_status1[inx_3] = dref_trigger_status10\n",
    "        index3=event_name+'_'+lead_time2+str(inx_3)\n",
    "        DREF_trigger_list_10[index3] = dref_trigger_status10\n",
    "        \n",
    "    \n",
    "    ### DREF trigger based on probability of Houses \n",
    "    agg_impact = probability_impact[\"Total_buildings_ML_Model\"].values\n",
    "    for key, values in dref_probabilities.items():\n",
    "        dref_trigger_status = {}\n",
    "        index2=index2+1\n",
    "\n",
    "        trigger_stat = 100*(sum(1.13*i > values[0] for i in agg_impact) /len(agg_impact))\n",
    "        \n",
    "        if  (values[1] < (sum(1.13*i > values[0] for i in agg_impact) /len(agg_impact))):\n",
    "            trigger_stat_=True\n",
    "            thershold=key \n",
    "        else:\n",
    "            trigger_stat_=False\n",
    "            thershold=key\n",
    "        \n",
    "        dref_trigger_status['thre_name'] = key\n",
    "        dref_trigger_status['thr_value'] = int(trigger_stat)      \n",
    "        dref_trigger_status['event'] = event_name \n",
    "        dref_trigger_status['lead_time'] = int(lead_time2)-10\n",
    "        dref_trigger_status['trigger_stat'] = trigger_stat_\n",
    "        index=event_name+'_'+lead_time2+str(index2)\n",
    "        \n",
    "        DREF_trigger_list[index] = dref_trigger_status\n",
    "    \n",
    "        \n",
    "    #DREF_trigger_list[event_name] = dref_trigger_status\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############ check trigger for START project\n",
    "    \n",
    "    start_trigger_status = {}\n",
    "    \n",
    "    provinces_names={'PH166700000':'SurigaoDeLnorte','PH021500000':'Cagayan','PH082600000':'EasternSamar'}\n",
    "    \n",
    "    for provinces in ['PH166700000','PH021500000','PH082600000']:\n",
    "        triggers=START_probabilities[provinces]\n",
    "        prov_name=provinces_names[provinces]\n",
    "        df_trig=IMPACT_DF12.query('Prov_Code==@provinces')\n",
    "        if not df_trig.empty:  \n",
    "            probability_impact=df_trig.groupby(['storm_id','typhoon_year','forecast_time']).agg(\n",
    "                                                Total_affected_ML_Model=('number_affected_pop__prediction', sum),\n",
    "                                                Total_buildings_ML_Model=('number_dmg_Build_prediction', sum)).sort_values(by='Total_buildings_ML_Model',ascending=False).reset_index()\n",
    "            \n",
    "                \n",
    "            ######## calculate probability for impact\n",
    "            \n",
    "            \n",
    "            agg_impact = probability_impact[\"Total_affected_ML_Model\"].values\n",
    "     \n",
    "            thershold='NAN'\n",
    "            \n",
    "            \n",
    "\n",
    "            for key, values in triggers.items():\n",
    "                inx_=inx_+1\n",
    "                start_trigger_status1 = {}\n",
    "                trigger_stat = 100*(sum(i > values[0] for i in agg_impact) /len(agg_impact)       )\n",
    "                #start_trigger_status1[key] = int(trigger_stat)\n",
    "                start_trigger_status1['thr_name']=key\n",
    "                start_trigger_status1['thr_value']=int(trigger_stat)\n",
    "\n",
    "                if  (values[1] < (sum(i > values[0] for i in agg_impact) /len(agg_impact))):\n",
    "                    trigger_stat_=True\n",
    "                    thershold=key \n",
    "                else:\n",
    "                    trigger_stat_=False\n",
    "                    thershold=key\n",
    "            #start_trigger_list.append({'event':event_name, 'province':prov_name,                                      'lead_time':lead_time2,                                      'status':trigger_stat_,                                      'threshold':thershold})\n",
    "            \n",
    "                #start_trigger_status['event'] = event_name\n",
    " \n",
    "\n",
    "                start_trigger_status1['PROVINCE'] = prov_name\n",
    "                start_trigger_status1['EVENT'] = event_name\n",
    "                start_trigger_status1['lead_time'] = lead_time2\n",
    "                start_trigger_status1['trigger_stat'] = trigger_stat_         \n",
    "                index=event_name+'_'+lead_time2+str(index2)\n",
    "                start_trigger_dic[index] = start_trigger_status1\n",
    "       \n",
    "            #start_trigger[prov_name] =start_trigger_status\n",
    "            \n",
    "            #start_trigger_list.append(start_trigger_status)\n",
    "    #start_trigger_list[event_name] = start_trigger_status\n",
    "     \n",
    "\n",
    "Trigger_status=True\n",
    "impact_path_ = os.path.join(wor_dir,\"IBF-typhoon-model/data/results/dref_trigger_table_10%.csv\")\n",
    "pd.DataFrame.from_dict(DREF_trigger_list_10,orient='index').query('trigger_stat==@Trigger_status').to_csv(impact_path_) \n",
    "\n",
    "impact_path_ = os.path.join(wor_dir,\"IBF-typhoon-model/data/results/dref_trigger_table_prob.csv\")\n",
    "pd.DataFrame.from_dict(DREF_trigger_list,orient='index').query('trigger_stat==@Trigger_status').to_csv(impact_path_) \n",
    "\n",
    "impact_path_ = os.path.join(wor_dir,\"IBF-typhoon-model/data/results/start_trigger_table_prob.csv\")\n",
    "pd.DataFrame.from_dict(start_trigger_dic,orient='index').query('trigger_stat==@Trigger_status').to_csv(impact_path_) \n",
    "\n",
    " \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('geo_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c56dc72bed4e123c21144b1399355bbd07b8d587f6360d6b6ea22ee2ab335a35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
