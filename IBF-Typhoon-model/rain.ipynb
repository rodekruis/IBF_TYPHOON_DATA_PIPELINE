{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import geopandas as gpd\n",
    "\"\"\"\n",
    "download rainfall \n",
    "\"\"\"\n",
    "ADMIN_PATH = 'data/gis_data/phl_admin3_simpl2.geojson'\n",
    "list_df=[]  #to store final rainfall dataframes \n",
    "#ADMIN_PATH = 'data/gis_data/phl_admin3_simpl2.geojson'\n",
    "admin = gpd.read_file(ADMIN_PATH)\n",
    "\n",
    "\n",
    "RAINFALL_TIME_STEP=['06', '24']\n",
    "\n",
    "url_base = 'https://nomads.ncep.noaa.gov/pub/data/nccf/com/naefs/prod/gefs.'\n",
    "\n",
    "#url1 = f\"{url_base}{data_point}/\"  # Use the timestamp of the input folder for the query\n",
    "#url2 = f\"{url_base}{Alternative_data_point}/\"  # Yesterday's date\n",
    "import datetime as dt\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "start_time = dt.datetime.now()\n",
    "from rasterstats import zonal_stats\n",
    "no_data_value=29999\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import requests\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def url_is_alive(url):\n",
    "    \"\"\"\n",
    "    Checks that a given URL is reachable.\n",
    "    :param url: A URL\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    request = urllib.request.Request(url)\n",
    "    request.get_method = lambda: 'HEAD'\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlopen(request)\n",
    "        return True\n",
    "    except urllib.error.HTTPError:\n",
    "        return False\n",
    "    \n",
    "def get_grib_files(url, path, rainfall_path, use_cache=True):\n",
    "    base_urls = []\n",
    "    for items in url: #listFD(url):\n",
    "        if url_is_alive(items+'prcp_bc_gb2/'):\n",
    "            base_urls.append(items)\n",
    "    base_url = base_urls[-1]\n",
    "    base_url_hour = base_url+'prcp_bc_gb2/geprcp.t%sz.pgrb2a.0p50.bc_' % base_url.split('/')[-2]\n",
    "    time_step_list1 = [ '24', '30', '36', '42', '48', '54', '60', '66', '72']\n",
    "    time_step_list = ['06', '12', '18', '24', '30', '36', '42', '48', '54', '60', '66', '72']\n",
    "    rainfall_24 = [base_url_hour+'24hf0%s' % t for t in time_step_list1]\n",
    "    rainfall_06 = [base_url_hour+'06hf0%s' % t for t in time_step_list]\n",
    "    for rain_file in rainfall_06 + rainfall_24:\n",
    "        output_file = os.path.join(os.path.relpath(rainfall_path, path), rain_file.split('/')[-1]+'.grib2')\n",
    "        if use_cache and os.path.isfile(output_file):\n",
    "      \n",
    "            continue\n",
    "        try:\n",
    "            with urllib.request.urlopen(rain_file) as response, open(output_file, 'wb') as out_file:\n",
    "\n",
    "                shutil.copyfileobj(response, out_file)\n",
    "        except urllib.error.HTTPError:\n",
    "\n",
    "            continue\n",
    "\n",
    " \n",
    "data_point = start_time.strftime(\"%Y%m%d\") \n",
    "MAIN_DIRECTORY ='./'\n",
    "rainfall_path =MAIN_DIRECTORY+ 'forecast/rainfall/' \n",
    "Alternative_data_point= data_point\n",
    "\n",
    "try:\n",
    "\n",
    "    url1=[f'{url_base}{data_point}/{hh}/' for hh in ['00','06','12','18']]\n",
    "    get_grib_files(url1, MAIN_DIRECTORY, rainfall_path)\n",
    "except IndexError:\n",
    "    # If list index out of range then it means that there are no files available,\n",
    "    # use tomorrow's date instead\n",
    "\n",
    "    url2=[f'{url_base}{Alternative_data_point}/{hh}/' for hh in ['00','06','12','18']]\n",
    "    get_grib_files(url2, MAIN_DIRECTORY, rainfall_path)\n",
    "\n",
    "for hour in RAINFALL_TIME_STEP:\n",
    "    pattern = f'.pgrb2a.0p50.bc_{hour}h'\n",
    "    output_filename = f'rainfall_{hour}.nc'\n",
    "    filename_list = Path(rainfall_path).glob(f'*{pattern}*')\n",
    "    with xr.open_mfdataset(filename_list, engine='cfgrib',\n",
    "                            combine=\"nested\", concat_dim=[\"time\"],\n",
    "                            backend_kwargs={\"indexpath\": \"\",\n",
    "                                            'filter_by_keys': {'totalNumber': 30}}\n",
    "                            ) as ds:\n",
    "        filepath = os.path.join(rainfall_path, output_filename)\n",
    "    \n",
    "        ds = ds.median(dim='number') #store only the median of the ensemble members\n",
    "        ds.to_netcdf(filepath)\n",
    "    #zonal stats to calculate rainfall per manucipality \n",
    "    #list_df.append(zonal_stat_rain(filepath,admin))  \n",
    "    rain_6h=rasterio.open(filepath) \n",
    "    band_indexes = rain_6h.indexes\n",
    "    transform = rain_6h.transform\n",
    "    all_band_summaries = []\n",
    "    for b in band_indexes:\n",
    "        array = rain_6h.read(b)\n",
    "        band_summary = zonal_stats(\n",
    "            admin,\n",
    "            array,\n",
    "            prefix=f\"band{b}_\",\n",
    "            stats=\"mean\",\n",
    "            nodata=no_data_value,\n",
    "            all_touched=True,\n",
    "            affine=transform,\n",
    "        )\n",
    "        all_band_summaries.append(band_summary)\n",
    "    # Flip dimensions\n",
    "    shape_summaries = list(zip(*all_band_summaries))\n",
    "    # each list entry now reflects a municipalities, and consists of a dictionary with the rainfall in mm / 6h for each time frame\n",
    "    final = [{k: v for d in s for k, v in d.items()} for s in shape_summaries]\n",
    "    # Obtain list with maximum 6h rainfall\n",
    "    maximum_6h = [max(x.values()) for x in final]\n",
    "    list_df.append(pd.DataFrame(maximum_6h))\n",
    "df_rain = pd.concat(list_df,axis=1, ignore_index=True) \n",
    "df_rain.columns = [\"max_\"+time_itr+\"h_rain\" for time_itr in RAINFALL_TIME_STEP]\n",
    "df_rain['Mun_Code']=list(admin['adm3_pcode'].values)\n",
    "\n",
    "df_rain.to_csv(os.path.join(rainfall_path, \"rain_data.csv\"), index=False)\n",
    "    \n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tyworkflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
